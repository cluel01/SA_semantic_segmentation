{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "from config import UNetTraining\n",
    "config = UNetTraining.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    %run \"1_data_preparation.ipynb\"\n",
    "    %run \"2_model.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Define callbacks for the early stopping of training, LearningRateScheduler and model checkpointing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(config.model_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = False)\n",
    "\n",
    "#reduceonplatea; It can be useful when using adam as optimizer\n",
    "#Reduce learning rate when a metric has stopped improving (after some patience,reduce by a factor of 0.33, new_lr = lr * factor).\n",
    "#cooldown: number of epochs to wait before resuming normal operation after lr has been reduced.\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.33,\n",
    "                                   patience=4, verbose=1, mode='min',\n",
    "                                   min_delta=0.0001, cooldown=4, min_lr=1e-16)\n",
    "\n",
    "#early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=2, patience=15)\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "log_dir = os.path.join('./logs','UNet_{}_{}_{}_{}_{}'.format(timestr,OPTIMIZER_NAME,LOSS_NAME,chs, config.input_shape[0]))\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "callbacks_list = [checkpoint, tensorboard] #reduceLROnPlat is not required with adaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_history = model.fit(datagen.flow(X_train_mini_np, y_train_mini_np,\n",
    "                                batch_size=config.BATCH_SIZE, subset='training'),\n",
    "                        epochs=config.NB_EPOCHS,\n",
    "                        steps_per_epoch=len(X_train_mini_np) // config.BATCH_SIZE, #config.MAX_TRAIN_STEPS, # steps_per_epoch * epochs <= # data instances = 94\n",
    "                        validation_data=datagen.flow(X_val_np, y_val_np, \n",
    "                                batch_size=config.BATCH_SIZE, subset='validation'),\n",
    "                        validation_steps=len(X_train_mini_np) // config.BATCH_SIZE,\n",
    "                        batch_size=config.BATCH_SIZE,\n",
    "                        callbacks=callbacks_list\n",
    "                        # use_multiprocessing=True # the generator is not very thread safe\n",
    "                )\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(config.model_path):\n",
    "    os.makedirs(config.model_path)\n",
    "acc     = round(100*loss_history.history['accuracy'][-1], 4)\n",
    "val_acc = round(100*loss_history.history['val_accuracy'][-1], 4)\n",
    "model_path = os.path.join(config.model_path,'UNet_{}_{}_{}_{}_{}_{}.h5'.format(timestr,OPTIMIZER_NAME,LOSS_NAME,config.input_shape[0], acc, val_acc))\n",
    "print(model_path)\n",
    "model.save(model_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
